"""
jira_extract_urls_to_json.py

Usage:
    - put this file next to your internal.py (which contains the Jira class you shared)
    - pip install requests beautifulsoup4
    - edit `browse_urls` list or load from a file
    - run: python jira_extract_urls_to_json.py

Output:
    - jira_issue_urls.json  (mapping issue_key -> [unique urls])
"""

import re
import json
from pathlib import Path
from typing import Any, Set
from bs4 import BeautifulSoup

# import your existing Jira class
from internal import Jira

# -------- config --------
# list of browse URLs you want to process (or load from file)
browse_urls = [
    "https://wpb-jira.systems.uk.hsbc/browse/ABC-123456",
    # add more...
]

OUTPUT_FILE = "jira_issue_urls.json"

# -------- regexes & helpers --------
URL_REGEX = re.compile(r'https?://[^\s)>\]\["\']+')
MD_LINK_REGEX = re.compile(r'\[.*?\]\((https?://[^\s)]+)\)')
ANGLE_LINK_REGEX = re.compile(r'<(https?://[^>]+)>')

def extract_urls_from_html(html: str) -> Set[str]:
    """Parse HTML (or HTML-like) and extract href/src and fallback to regex."""
    found = set()
    if not html:
        return found
    try:
        soup = BeautifulSoup(html, "html.parser")
    except Exception:
        soup = None

    if soup:
        # <a href="">
        for a in soup.find_all("a", href=True):
            href = a["href"].strip()
            if href:
                found.add(href)
        # attributes that might contain URLs
        for tag in soup.find_all(True):
            for attr in ("src", "data-src", "data-href", "content"):
                if tag.has_attr(attr):
                    val = tag.get(attr)
                    if isinstance(val, str) and val.startswith("http"):
                        found.add(val.strip())

    # regex fallback for URLs in text or markdown inside the HTML string
    for u in URL_REGEX.findall(html):
        found.add(u.strip())
    for u in MD_LINK_REGEX.findall(html):
        found.add(u.strip())
    for u in ANGLE_LINK_REGEX.findall(html):
        found.add(u.strip())

    return found

def extract_urls_from_text(text: str) -> Set[str]:
    """Extract URLs from plain text and markdown-like link patterns."""
    found = set()
    if not text:
        return found
    for u in URL_REGEX.findall(text):
        found.add(u.strip())
    for u in MD_LINK_REGEX.findall(text):
        found.add(u.strip())
    for u in ANGLE_LINK_REGEX.findall(text):
        found.add(u.strip())
    return found

def scan_attachment_for_urls(att: dict) -> Set[str]:
    """Check common attachment fields for URLs."""
    urls = set()
    if not isinstance(att, dict):
        return urls
    for key in ("content", "self", "thumbnail", "url", "download"):
        v = att.get(key)
        if isinstance(v, str) and v.startswith("http"):
            urls.add(v.strip())
    # scan nested strings
    for v in att.values():
        if isinstance(v, str):
            urls.update(extract_urls_from_text(v))
        elif isinstance(v, (dict, list)):
            urls.update(recursive_scan_for_urls(v))
    return urls

def recursive_scan_for_urls(obj: Any) -> Set[str]:
    """Walk any JSON-like structure and extract URLs from strings/HTML."""
    found = set()
    if obj is None:
        return found
    if isinstance(obj, str):
        found.update(extract_urls_from_html(obj))
        found.update(extract_urls_from_text(obj))
        return found
    if isinstance(obj, dict):
        for k, v in obj.items():
            if k.lower() in ("attachment", "attachments") and isinstance(v, list):
                for att in v:
                    found.update(scan_attachment_for_urls(att))
            else:
                found.update(recursive_scan_for_urls(v))
        return found
    if isinstance(obj, list):
        for item in obj:
            found.update(recursive_scan_for_urls(item))
        return found
    return found

# -------- top-level orchestration --------

def extract_issue_key_from_browse_url(browse_url: str) -> str:
    return browse_url.rstrip("/").split("/")[-1]

def normalize_url(u: str) -> str:
    return u.strip()

def process_issue(jira: Jira, browse_url: str):
    key = extract_issue_key_from_browse_url(browse_url)
    print(f"Processing {key} ...")
    raw = jira.get_ori(key)

    # jira.get_ori in your screenshot returned .text; handle both str and dict
    if isinstance(raw, str):
        try:
            issue_json = json.loads(raw)
        except json.JSONDecodeError:
            # If the response is a text wrapper like "issues": [...], try to be forgiving
            print(f"Warning: response for {key} is not JSON. Saving raw text and returning empty list.")
            # optionally save raw to disk for debugging
            Path(f"{key}_raw_response.txt").write_text(raw, encoding="utf-8")
            return key, []
        else:
            # loaded JSON
            resp_obj = issue_json
    elif isinstance(raw, dict):
        resp_obj = raw
    else:
        print(f"Unexpected response type for {key}: {type(raw)}")
        return key, []

    # The jql endpoint likely returns a wrapper with "issues": [ ... ]
    if isinstance(resp_obj, dict) and resp_obj.get("issues"):
        issue_obj = resp_obj["issues"][0]
    else:
        # maybe the API returned the issue object directly
        issue_obj = resp_obj

    urls = set()

    # Prefer renderedFields if present (HTML)
    rendered = issue_obj.get("renderedFields") or {}
    if rendered:
        desc_html = rendered.get("description")
        urls.update(extract_urls_from_html(desc_html))
        # comments rendered
        cblock = rendered.get("comment") or {}
        for c in cblock.get("comments", []) if isinstance(cblock, dict) else []:
            body = c.get("body") or c.get("renderedBody") or ""
            urls.update(extract_urls_from_html(body))

    # fields.* explicit checks
    fields = issue_obj.get("fields") or {}
    # description (raw)
    urls.update(extract_urls_from_html(fields.get("description", "")))
    # comments raw
    comments_raw = fields.get("comment", {}).get("comments", []) if isinstance(fields.get("comment", {}), dict) else []
    for c in comments_raw:
        body = c.get("body", "")
        urls.update(extract_urls_from_html(body))
        urls.update(extract_urls_from_text(body))
    # attachments
    for att in fields.get("attachment", []) if isinstance(fields.get("attachment", []), list) else []:
        urls.update(scan_attachment_for_urls(att))

    # fallback: scan everything
    urls.update(recursive_scan_for_urls(issue_obj))

    # normalize, dedupe, optionally filter out internal jira API links
    normalized = sorted({normalize_url(u) for u in urls if isinstance(u, str) and u.strip()})

    # optional: filter out internal jira API/self links (uncomment to enable)
    # normalized = [u for u in normalized if "wpb-jira.systems.uk.hsbc/rest/api" not in u]

    return key, normalized

def main():
    jira = Jira()
    final = {}

    for browse in browse_urls:
        key, urls = process_issue(jira, browse)
        final[key] = urls
        print(f" -> {len(urls)} urls found for {key}")

    # save final mapping
    with open(OUTPUT_FILE, "w", encoding="utf-8") as fh:
        json.dump(final, fh, indent=2, ensure_ascii=False)

    print(f"\nDone â€” saved results to {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
