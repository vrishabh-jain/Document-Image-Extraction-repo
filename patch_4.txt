# --- place near top of file (if you prefer, inside extract_and_save_links so it has access to self.session) ---
import re
from urllib.parse import urlparse, parse_qs

def looks_like_file_link(url, anchor_tag=None, session=None, timeout=6.0):
    """
    Heuristic checks to decide if a link is a file/attachment.
    - anchor_tag: optional BeautifulSoup tag, used to inspect data-* attributes or onclick
    - session: optional requests.Session (used for HEAD/GET fallback)
    """
    # 1) fast extension check (existing)
    try:
        if is_file_link(url):
            return True
    except Exception:
        pass

    # parsed path / query inspection
    p = urlparse(url)
    path = (p.path or "").lower()
    qs = parse_qs(p.query or "")

    # 2) common filename query-params or download words in path
    if any(k in qs for k in ("filename", "file", "attachment", "name")):
        return True
    if re.search(r'/download/|/attachments/|/attach|/attachment|/rest/api/content/.*/child/attachment|/pages/viewpageattachments.action', path, re.I):
        return True
    # some systems use a numeric id path with /files/ or /content/...
    if re.search(r'/files/|/file/|/downloadAttachment', path, re.I):
        return True

    # 3) inspect anchor attributes (some Confluence markup)
    if anchor_tag is not None:
        # data-file-name, data-filename, data-linked-resource-type etc
        for attr in ("data-file-name", "data-filename", "data-attachment", "data-linked-resource", "data-linked-resource-type"):
            val = anchor_tag.get(attr)
            if val:
                return True
        # onclick patterns like "AJS.dialog2(...openAttachment...)" or other JS that references attachments
        onclick = anchor_tag.get("onclick", "") or ""
        if re.search(r'attachment|download|openAttachment|file|getAttachment', onclick, re.I):
            return True

    # 4) fallback: HEAD -> check Content-Type / Content-Disposition
    #    (use session if provided so auth/headers are honored)
    if session is not None:
        try:
            resp = session.head(url, allow_redirects=True, timeout=timeout)
            # Some servers block HEAD; if status is >=400 try GET as last resort
            if resp.status_code < 400:
                ctype = (resp.headers.get("Content-Type") or "").lower()
                cdisp = resp.headers.get("Content-Disposition", "") or ""
                if ctype and (ctype.startswith("application/") or ctype.startswith("image/") or "octet-stream" in ctype):
                    # debug print
                    print(f"[file-detect][HEAD] {url} -> Content-Type: {ctype}, Content-Disposition: {cdisp}")
                    return True
                if "filename=" in cdisp.lower():
                    print(f"[file-detect][HEAD] {url} -> Content-Disposition: {cdisp}")
                    return True
            # if HEAD failed or not decisive, fall through to GET
        except Exception:
            # silent - network/auth might block HEAD; we'll try GET below as last resort
            pass

        # Last resort: GET (very conservative)
        try:
            resp = session.get(url, allow_redirects=True, stream=True, timeout=timeout)
            # check headers
            ctype = (resp.headers.get("Content-Type") or "").lower()
            cdisp = resp.headers.get("Content-Disposition", "") or ""
            if ctype and (ctype.startswith("application/") or ctype.startswith("image/") or "octet-stream" in ctype):
                print(f"[file-detect][GET] {url} -> Content-Type: {ctype}, Content-Disposition: {cdisp}")
                resp.close()
                return True
            if "filename=" in cdisp.lower():
                print(f"[file-detect][GET] {url} -> Content-Disposition: {cdisp}")
                resp.close()
                return True
            # If content looks like HTML, probably not a file
            # Close the streamed response immediately
            resp.close()
        except Exception:
            # ignore network errors - treat as non-file
            pass

    # default: not a file
    return False



# ensure top-level files list exists
results.setdefault("links", {}).setdefault("files", [])

for a in soup.find_all("a", href=True):
    href = a["href"].strip()
    abs_url = urljoin(self.base_url, href)

    # classify files first with rich heuristics (pass anchor tag + session so auth works)
    if looks_like_file_link(abs_url, anchor_tag=a, session=getattr(self, "session", None)):
        # avoid duplicates
        if abs_url not in results["links"]["files"]:
            results["links"]["files"].append(abs_url)
        # do not also classify as external/jira/confluence
        continue

    # previous classification logic
    low = abs_url.lower()
    if "jira" in low:
        if abs_url not in results["links"]["jira"]:
            results["links"]["jira"].append(abs_url)
    elif "confluence" in low:
        if abs_url not in results["links"]["confluence"]:
            results["links"]["confluence"].append(abs_url)
    else:
        if abs_url not in results["links"]["external"]:
            results["links"]["external"].append(abs_url)
