import os
import json
import re
import time
import pathlib
import traceback
from urllib.parse import urljoin, urlparse
import requests
from bs4 import BeautifulSoup

from config_provider import ConfigProvider
from get_confluence_content import GetConfluenceContent

FILE_EXTS = re.compile(
    r"\.(pdf|docx|doc|xlsx|xls|pptx|ppt|odt|rtf|txt|zip|tar|gz|xlsm)$",
    re.IGNORECASE,
)


def _normalize_url(base: str, href: str) -> str:
    if not href:
        return ""
    full = urljoin(base, href) if base and not urlparse(href).scheme else href
    return urlparse(full)._replace(fragment="").geturl()


class DownloadAttachments:
    def __init__(self, confluence_url, page_id, verify: bool = False):
        self.confluence_url = confluence_url
        self.page_id = page_id
        self.verify = verify
        self.config_provider = ConfigProvider()
        self.confluence_content = GetConfluenceContent(
            confluence_url, self.config_provider
        )

    def _classify_link(self, url: str) -> str:
        if FILE_EXTS.search(urlparse(url).path or ""):
            return "files"
        u = url.lower()
        if "jira" in u or "/browse/" in u or "/issues/" in u:
            return "jira"
        if self.confluence_url in url or u.startswith("/wiki") or u.startswith("/pages"):
            return "confluence"
        return "external"

    def _detect_content_area(self, soup: BeautifulSoup):
        # Look for common Confluence/Jira page containers
        candidates = [
            ("id", "main-content"),
            ("id", "content"),
            ("id", "wiki-content"),
            ("class", "wiki-content"),
            ("class", "ak-renderer-document"),
            ("class", "page-content"),
            ("id", "descriptionmodule"),
            ("id", "issue-content"),
            ("id", "content-body"),
            ("class", "editor-content"),
        ]
        for kind, val in candidates:
            node = soup.find("div", {kind: val})
            if node:
                return node
        return soup

    def _crawl_nested_links(self, results: dict, timeout: float = 8.0, delay: float = 0.15):
        """
        Crawl only jira + confluence seeds.
        Saves results['nested_links'][parent] with the same categories: jira, confluence, external, files.
        """
        results.setdefault("nested_links", {})
        seeds = results["links"]["jira"] + results["links"]["confluence"]
        if not seeds:
            return results

        headers = self.config_provider.get_headers()
        session = requests.Session()
        session.headers.update(headers)

        for parent in seeds:
            nested = {"jira": [], "confluence": [], "external": [], "files": []}
            try:
                r = session.get(parent, headers=headers, timeout=timeout, verify=self.verify)
                r.raise_for_status()
                if "html" not in (r.headers.get("Content-Type") or "").lower():
                    results["nested_links"][parent] = nested
                    continue

                soup = BeautifulSoup(r.text, "html.parser")
                content_area = self._detect_content_area(soup)

                for a in content_area.find_all("a", href=True):
                    child = _normalize_url(parent, a["href"].strip())
                    if not child:
                        continue
                    cat = self._classify_link(child)
                    if child not in nested[cat]:
                        nested[cat].append(child)
                    if cat == "files" and child not in results["links"]["files"]:
                        results["links"]["files"].append(child)

                results["nested_links"][parent] = nested
            except Exception:
                traceback.print_exc()
                results["nested_links"][parent] = nested
            finally:
                time.sleep(delay)

        return results

    def extract_and_save_links(self, dest_folder: str, page_id: str):
        """
        Scan page HTML for links. Categorize into jira, confluence, external, files.
        Crawl nested jira/confluence links.
        Save results JSON.
        """
        results = {"links": {"jira": [], "confluence": [], "external": [], "files": []}}

        # get page HTML
        page = self.confluence_content.get_page_as_json(page_id)
        if not page:
            return results
        page_html = page["body"]["storage"]["value"]

        soup = BeautifulSoup(page_html, "html.parser")

        # extract top-level links
        for a in soup.find_all("a", href=True):
            full_url = _normalize_url(self.confluence_url, a["href"].strip())
            cat = self._classify_link(full_url)
            if full_url not in results["links"][cat]:
                results["links"][cat].append(full_url)

        # nested crawl
        self._crawl_nested_links(results)

        # save JSON
        pathlib.Path(dest_folder).mkdir(parents=True, exist_ok=True)
        json_path = os.path.join(dest_folder, f"{page_id}_results.json")
        with open(json_path, "w", encoding="utf-8") as jf:
            json.dump(results, jf, indent=4, ensure_ascii=False)

        return results
