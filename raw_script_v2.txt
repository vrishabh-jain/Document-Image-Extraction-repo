import fitz  # PyMuPDF
import os
import json
import psycopg2
import asyncio
import base64
from PIL import Image
import io

from dotenv import load_dotenv
load_dotenv()

import docx
from zipfile import ZipFile
from pptx import Presentation
from bs4 import BeautifulSoup
import requests
import tempfile
import shutil
import statistics
from urllib.parse import urlparse

# ======================
# Azure / DB integration (from your provided snippet)
# ======================
with open(f"{os.getcwd()}{os.sep}cred.json", "r") as f:
    cred = json.load(f)

def get_token_v2():
    con = psycopg2.connect(
        **cred["db"]
    )

    with con.cursor() as cur:
        spn = None
        try:
            cur.execute(
                f"Select value, last_update_time from meta where key='spn';"
            )

            results = cur.fetchall()
            if len(results) > 0:
                spn, last_update_time = results[0]
            else:
                raise Exception(f"Failed to select")
        except Exception as e:
            import traceback
            traceback.print_exc()

            raise Exception(f"{e}")

    return spn

token = get_token_v2()

# NOTE: keep the same class name you used earlier for your Azure client.
# It must be available in the runtime environment where you run this script.
# Example: from some_azure_wrapper import AsyncAzureOpenAI
# Here we assume AsyncAzureOpenAI is available in the environment.
from some_azure_wrapper import AsyncAzureOpenAI  # <-- ensure this import matches your environment

openai_llm = AsyncAzureOpenAI(
    azure_ad_token=token,
    azure_endpoint="https://iwpb-aap-dev-wus.openai.azure.com/",
    azure_deployment="gpt-4.1-nano",
    api_version="2025-01-01-preview"
)

# ======================
# CONFIG
# ======================
DOC_PATH = r"D:/Vrishabh/Personal Projects/PDF Extraction/test_docs/sample_test.pptx"
OUTPUT_FILE = "extracted_texts.txt"
MODEL = "gpt-4.1-nano"  # use deployment/model identifier appropriate for your Azure setup

# ======================
# FUNCTIONS (unchanged extractors & helpers from your original script)
# ======================

def extract_figures_from_pdf(pdf_path, output_dir, min_size):
    if not isinstance(min_size, int) or min_size < 0:
        raise ValueError("min_size must be a non-negative integer")

    os.makedirs(output_dir, exist_ok=True)
    doc = fitz.open(pdf_path)
    image_data = []
    skipped = 0

    for page_index, page in enumerate(doc):
        images = page.get_images(full=True)
        for img_index, img in enumerate(images):
            xref = img[0]
            smask = img[1]

            try:
                base_pix = fitz.Pixmap(doc, xref)
            except Exception:
                continue

            if getattr(base_pix, "alpha", False):
                try:
                    base_pix_no_alpha = fitz.Pixmap(base_pix, 0)
                    base_pix = base_pix_no_alpha
                except Exception:
                    pass

            pix = base_pix
            if smask and smask > 0:
                try:
                    mask_pix = fitz.Pixmap(doc, smask)
                    try:
                        pix = fitz.Pixmap(base_pix, mask_pix)
                    except Exception:
                        pix = base_pix
                    finally:
                        mask_pix = None
                except Exception:
                    pix = base_pix

            try:
                if getattr(pix, "colorspace", None) and pix.colorspace.n > 3:
                    pix = fitz.Pixmap(fitz.csRGB, pix)
            except Exception:
                pass

            try:
                sample_len = len(pix.samples)
            except Exception:
                sample_len = 0

            if sample_len < min_size:
                skipped += 1
                pix = None
                base_pix = None
                continue

            temp_path = os.path.join(output_dir, f"temp_page_{page_index+1}_figure_{img_index+1}.png")
            try:
                pix.save(temp_path)
            except Exception:
                pix = None
                base_pix = None
                try:
                    if os.path.exists(temp_path):
                        os.remove(temp_path)
                except Exception:
                    pass
                continue

            try:
                pil_img = Image.open(temp_path)
            except Exception:
                try:
                    os.remove(temp_path)
                except Exception:
                    pass
                pix = None
                base_pix = None
                continue

            if pil_img.mode in ('RGBA', 'LA', 'P'):
                if pil_img.mode == 'P':
                    pil_img = pil_img.convert('RGBA')
                background = Image.new("RGB", pil_img.size, (255, 255, 255))
                try:
                    alpha = pil_img.split()[3] if pil_img.mode == 'RGBA' else pil_img.split()[1]
                    background.paste(pil_img, mask=alpha)
                except Exception:
                    background.paste(pil_img)
                pil_img = background

            img_ext = "jpeg"
            img_path = os.path.join(output_dir, f"page_{page_index+1}_figure_{img_index+1}.{img_ext}")
            try:
                pil_img.save(img_path, "JPEG", quality=95)
            except Exception:
                try:
                    os.remove(temp_path)
                except Exception:
                    pass
                pix = None
                base_pix = None
                continue

            try:
                with open(img_path, "rb") as f:
                    img_bytes = f.read()
            except Exception:
                img_bytes = b''

            image_data.append((img_path, img_bytes, img_ext, page_index))

            try:
                os.remove(temp_path)
            except Exception:
                pass

            pix = None
            base_pix = None

    doc.close()
    print(f"✅ Extracted {len(image_data)} figure images to {output_dir} (skipped {skipped} by min_size)")
    return image_data


def extract_figures_from_docx(docx_path, output_dir, min_size):
    if not isinstance(min_size, int) or min_size < 0:
        raise ValueError("min_size must be a non-negative integer")

    os.makedirs(output_dir, exist_ok=True)
    image_data = []
    skipped = 0

    try:
        with ZipFile(docx_path) as zipf:
            media_files = [name for name in zipf.namelist() if name.startswith('word/media/')]
            for img_index, media in enumerate(media_files, 1):
                try:
                    img_bytes = zipf.read(media)
                except Exception:
                    continue
                if len(img_bytes) < min_size:
                    skipped += 1
                    continue
                try:
                    pil_img = Image.open(io.BytesIO(img_bytes))
                except Exception:
                    continue

                if pil_img.mode in ('RGBA', 'LA', 'P'):
                    if pil_img.mode == 'P':
                        pil_img = pil_img.convert('RGBA')
                    background = Image.new("RGB", pil_img.size, (255, 255, 255))
                    try:
                        alpha = pil_img.split()[3] if pil_img.mode == 'RGBA' else pil_img.split()[1]
                        background.paste(pil_img, mask=alpha)
                    except Exception:
                        background.paste(pil_img)
                    pil_img = background

                img_ext = "jpeg"
                base_name = os.path.splitext(os.path.basename(media))[0]
                img_path = os.path.join(output_dir, f"docx_{base_name}_{img_index}.{img_ext}")
                try:
                    pil_img.save(img_path, "JPEG", quality=95)
                except Exception:
                    continue

                try:
                    with open(img_path, "rb") as f:
                        saved_bytes = f.read()
                except Exception:
                    saved_bytes = img_bytes  # fallback

                image_data.append((img_path, saved_bytes, img_ext, 0))
    except Exception as e:
        print("⚠️ DOCX extraction error:", e)

    print(f"✅ Extracted {len(image_data)} figure images to {output_dir} (skipped {skipped} by min_size)")
    return image_data


def extract_figures_from_pptx(pptx_path, output_dir, min_size):
    if not isinstance(min_size, int) or min_size < 0:
        raise ValueError("min_size must be a non-negative integer")

    os.makedirs(output_dir, exist_ok=True)
    image_data = []
    skipped = 0

    try:
        with ZipFile(pptx_path) as zipf:
            media_files = [name for name in zipf.namelist() if name.startswith('ppt/media/')]
            for img_index, media in enumerate(media_files, 1):
                try:
                    img_bytes = zipf.read(media)
                except Exception:
                    continue
                if len(img_bytes) < min_size:
                    skipped += 1
                    continue
                try:
                    pil_img = Image.open(io.BytesIO(img_bytes))
                except Exception:
                    continue

                if pil_img.mode in ('RGBA', 'LA', 'P'):
                    if pil_img.mode == 'P':
                        pil_img = pil_img.convert('RGBA')
                    background = Image.new("RGB", pil_img.size, (255, 255, 255))
                    try:
                        alpha = pil_img.split()[3] if pil_img.mode == 'RGBA' else pil_img.split()[1]
                        background.paste(pil_img, mask=alpha)
                    except Exception:
                        background.paste(pil_img)
                    pil_img = background

                img_ext = "jpeg"
                base_name = os.path.splitext(os.path.basename(media))[0]
                img_path = os.path.join(output_dir, f"pptx_{base_name}_{img_index}.{img_ext}")
                try:
                    pil_img.save(img_path, "JPEG", quality=95)
                except Exception:
                    continue

                try:
                    with open(img_path, "rb") as f:
                        saved_bytes = f.read()
                except Exception:
                    saved_bytes = img_bytes

                image_data.append((img_path, saved_bytes, img_ext, 0))
    except Exception as e:
        print("⚠️ PPTX extraction error:", e)

    print(f"✅ Extracted {len(image_data)} figure images to {output_dir} (skipped {skipped} by min_size)")
    return image_data


def extract_figures_from_html(html_path, output_dir, min_size):
    if not isinstance(min_size, int) or min_size < 0:
        raise ValueError("min_size must be a non-negative integer")

    os.makedirs(output_dir, exist_ok=True)

    with open(html_path, 'r', encoding='utf-8') as f:
        soup = BeautifulSoup(f, 'html.parser')

    image_data = []
    base_dir = os.path.dirname(html_path)
    skipped = 0
    img_index = 0

    for img_tag in soup.find_all('img'):
        src = img_tag.get('src') or img_tag.get('data-src') or img_tag.get('data-original')
        if not src:
            continue

        if src.startswith('//'):
            src = 'https:' + src

        img_bytes = None
        if src.startswith('http://') or src.startswith('https://'):
            try:
                resp = requests.get(src, timeout=5)
                if resp.status_code == 200:
                    img_bytes = resp.content
            except Exception:
                img_bytes = None
        else:
            local_path = os.path.join(base_dir, src)
            if os.path.exists(local_path):
                try:
                    with open(local_path, 'rb') as lf:
                        img_bytes = lf.read()
                except Exception:
                    img_bytes = None

        if not img_bytes:
            continue

        if len(img_bytes) < min_size:
            skipped += 1
            continue

        try:
            pil_img = Image.open(io.BytesIO(img_bytes))
        except Exception:
            continue

        if pil_img.mode in ('RGBA', 'LA', 'P'):
            if pil_img.mode == 'P':
                pil_img = pil_img.convert('RGBA')
            background = Image.new("RGB", pil_img.size, (255, 255, 255))
            try:
                alpha = pil_img.split()[3] if pil_img.mode == 'RGBA' else pil_img.split()[1]
                background.paste(pil_img, mask=alpha)
            except Exception:
                background.paste(pil_img)
            pil_img = background

        img_ext = "jpeg"
        img_index += 1
        img_path = os.path.join(output_dir, f"html_image_{img_index}.{img_ext}")
        try:
            pil_img.save(img_path, "JPEG", quality=95)
        except Exception:
            continue

        try:
            with open(img_path, "rb") as f:
                saved_bytes = f.read()
        except Exception:
            saved_bytes = img_bytes

        image_data.append((img_path, saved_bytes, img_ext, 0))

    print(f"✅ Extracted {len(image_data)} figure images to {output_dir} (skipped {skipped} by min_size)")
    return image_data


def get_context(doc, context_id, ext):
    if ext == '.pdf':
        page = doc[context_id]
        return page.get_text()
    elif ext == '.docx':
        return '\n'.join([p.text for p in doc.paragraphs])
    elif ext == '.pptx':
        text = ''
        for slide in doc.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    text += shape.text + '\n'
        return text
    elif ext == '.html':
        return doc.get_text(separator='\n')
    else:
        return ''


# ======================
# Replaced Gemini -> Azure OpenAI integration
# ======================

def _build_prompt_for_image(page_text):
    prompt = f"Here is the text from the page containing this image:\n{page_text}\n\nExtract all visible text from the image and provide a detailed description of this image in the context of the document and the surrounding page text. If it's a diagram or chart, describe relationships, labels, structure, and what it illustrates in the context of the document."
    return prompt


async def _azure_generate_with_image_async(prompt_text, img_bytes, img_ext):
    """
    Async helper that calls the AsyncAzureOpenAI client's generation method.
    This wrapper encodes the image in base64 and sends it along with the text prompt.
    The exact call signature used here follows a reasonable expectation for an async client;
    adapt if your AsyncAzureOpenAI client expects a different method / parameters.
    """
    mime_type = f"image/{img_ext}"
    # Prepare payload - many Azure wrappers accept messages or content pieces.
    # Here we pass a structure with text + base64 image blob.
    payload = {
        "model": MODEL,
        "input": [
            {"role": "user", "content": prompt_text},
            {"type": "image", "mime_type": mime_type, "data_base64": base64.b64encode(img_bytes).decode()}
        ]
    }

    # Call the async client's generate method. Adjust if your client uses a different method name.
    # We're using `generate_content` here to mirror the earlier Gemini call shape.
    resp = await openai_llm.generate_content(payload)
    # We expect resp to have a `text` attribute or similar. If not, adapt accordingly.
    # Return textual result (guarding for attribute differences).
    if hasattr(resp, "text"):
        return resp.text
    elif isinstance(resp, dict):
        # common shape: {"choices":[{"message":{"content":"..."}}]}
        # attempt to extract common fields conservatively
        choices = resp.get("choices") or []
        if choices:
            first = choices[0]
            msg = first.get("message") or first.get("delta") or first
            if isinstance(msg, dict):
                return msg.get("content") or msg.get("text") or str(resp)
        return str(resp)
    else:
        return str(resp)


def gemini_describe(img_bytes, img_ext, page_text):
    """Replaced Gemini call with Azure OpenAI async call (synchronous wrapper)."""
    prompt = _build_prompt_for_image(page_text)
    # Run the async helper synchronously
    try:
        result = asyncio.run(_azure_generate_with_image_async(prompt, img_bytes, img_ext))
    except Exception as e:
        # If async run fails, raise to surface issues like incorrect client API usage.
        raise

    return result.strip() if result else ""


# ======================
# The rest of the original script: helpers and process_document
# ======================

def _download_url_to_tempfile(url, timeout=10):
    try:
        head = requests.head(url, allow_redirects=True, timeout=5)
    except Exception:
        head = None

    content_type = None
    if head is not None and 'content-type' in head.headers:
        content_type = head.headers['content-type']

    if not content_type:
        resp = requests.get(url, stream=True, timeout=timeout)
    else:
        resp = requests.get(url, stream=True, timeout=timeout)

    if resp.status_code >= 400:
        raise ValueError(f"Failed to fetch URL: {url} (status {resp.status_code})")

    ct = resp.headers.get('content-type', content_type or '').lower()

    if 'html' in ct:
        ext = '.html'
    elif 'pdf' in ct:
        ext = '.pdf'
    elif 'presentation' in ct or 'pptx' in ct:
        ext = '.pptx'
    elif 'word' in ct or 'officedocument.wordprocessingml.document' in ct or 'msword' in ct:
        ext = '.docx'
    elif 'image' in ct:
        subtype = ct.split('/')[-1].split(';')[0] if '/' in ct else 'jpeg'
        ext = f".{subtype}" if not subtype.startswith('x-') else f".{subtype.replace('x-', '')}"
    else:
        path = urlparse(url).path
        _, ext = os.path.splitext(path)
        if not ext:
            ext = '.html'

    tmp_fd, tmp_path = tempfile.mkstemp(suffix=ext)
    os.close(tmp_fd)

    with open(tmp_path, 'wb') as out_f:
        for chunk in resp.iter_content(chunk_size=8192):
            if chunk:
                out_f.write(chunk)

    return tmp_path, ext


def determine_min_size(file_path, ext, mode='balanced', sample_limit=8, min_absolute=5000):
    sizes = []
    try:
        if ext == '.pdf':
            doc = fitz.open(file_path)
            for page_index, page in enumerate(doc):
                if len(sizes) >= sample_limit:
                    break
                for img in page.get_images(full=True):
                    if len(sizes) >= sample_limit:
                        break
                    xref = img[0]
                    try:
                        pix = fitz.Pixmap(doc, xref)
                        sizes.append(len(pix.samples))
                        pix = None
                    except Exception:
                        continue
            doc.close()

        elif ext == '.docx':
            try:
                doc = docx.Document(file_path)
                for rel in list(doc.part.rels.values()):
                    if len(sizes) >= sample_limit:
                        break
                    if "image" in rel.reltype:
                        try:
                            blob = rel.target_part.blob
                            sizes.append(len(blob))
                        except Exception:
                            continue
            except Exception:
                pass

        elif ext == '.pptx':
            try:
                with ZipFile(file_path) as zipf:
                    media_files = [name for name in zipf.namelist() if name.startswith('ppt/media/')]
                    for media in media_files[:sample_limit]:
                        try:
                            b = zipf.read(media)
                            sizes.append(len(b))
                        except Exception:
                            continue
            except Exception:
                pass

        elif ext in ('.html', '.htm'):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    soup = BeautifulSoup(f, 'html.parser')
                img_tags = soup.find_all('img')[:sample_limit]
                for tag in img_tags:
                    src = (tag.get('src') or tag.get('data-src') or tag.get('data-original') or tag.get('data-srcset') or '').strip()
                    if not src:
                        continue
                    if src.startswith('//'):
                        src = 'https:' + src
                    if src.startswith('http'):
                        try:
                            head = requests.head(src, allow_redirects=True, timeout=3)
                            cl = head.headers.get('content-length')
                            if cl and cl.isdigit():
                                sizes.append(int(cl))
                                continue
                        except Exception:
                            pass
                        try:
                            resp = requests.get(src, stream=True, timeout=5)
                            if resp.status_code == 200:
                                sizes.append(len(resp.content))
                        except Exception:
                            continue
                    else:
                        local_path = os.path.join(os.path.dirname(file_path), src)
                        if os.path.exists(local_path):
                            try:
                                with open(local_path, 'rb') as lf:
                                    data = lf.read()
                                    sizes.append(len(data))
                            except Exception:
                                continue
            except Exception:
                pass

    except Exception:
        sizes = []

    if mode == 'aggressive':
        multiplier = 0.25
    elif mode == 'conservative':
        multiplier = 1.2
    else:
        multiplier = 0.5

    if sizes:
        med = int(statistics.median(sizes))
        computed = max(int(med * multiplier), min_absolute)
        computed = int(max(min_absolute, min(computed, max(200000, med * 4))))
        return computed
    else:
        fallback_map = {
            '.pdf': 50000,
            '.docx': 15000,
            '.pptx': 20000,
            '.html': 8000
        }
        return fallback_map.get(ext, min_absolute)


def process_document(file_path, determine_mode='balanced'):
    temp_file_to_remove = None
    is_url = isinstance(file_path, str) and file_path.lower().startswith(('http://', 'https://'))
    original_url = file_path if is_url else None

    if is_url:
        print(f"Detected URL. Downloading: {file_path}")
        try:
            tmp_path, tmp_ext = _download_url_to_tempfile(file_path)
        except Exception as e:
            raise ValueError(f"Failed to download URL: {file_path}. Error: {e}")
        temp_file_to_remove = tmp_path
        file_path_to_use = tmp_path
    else:
        file_path_to_use = file_path

    try:
        ext = os.path.splitext(file_path_to_use)[1].lower()

        base_output_dir = f"extracted_images_{ext[1:] if ext else 'unknown'}"

        if is_url:
            parsed = urlparse(file_path)
            url_basename = os.path.basename(parsed.path) or parsed.netloc
            file_folder = os.path.splitext(url_basename)[0]
            file_folder = "".join(c for c in file_folder if c.isalnum() or c in ('_', '-')).strip() or "downloaded_file"
        else:
            file_folder = os.path.splitext(os.path.basename(file_path_to_use))[0]

        output_dir = os.path.join(base_output_dir, file_folder)
        os.makedirs(output_dir, exist_ok=True)

        output_file = os.path.join(output_dir, f"extracted_texts_{file_folder}.txt")

        min_size = determine_min_size(file_path_to_use, ext, mode=determine_mode, sample_limit=8, min_absolute=5000)
        print(f"Using min_size={min_size} bytes for extraction (mode={determine_mode}).")

        if ext == '.pdf':
            image_data = extract_figures_from_pdf(file_path_to_use, output_dir, min_size=min_size)
            doc = fitz.open(file_path_to_use)
        elif ext == '.docx':
            image_data = extract_figures_from_docx(file_path_to_use, output_dir, min_size=min_size)
            doc = docx.Document(file_path_to_use)
        elif ext == '.pptx':
            image_data = extract_figures_from_pptx(file_path_to_use, output_dir, min_size=min_size)
            doc = Presentation(file_path_to_use)
        elif ext in ('.html', '.htm'):
            image_data = extract_figures_from_html(file_path_to_use, output_dir, min_size=min_size)
            with open(file_path_to_use, 'r', encoding='utf-8') as f:
                doc = BeautifulSoup(f, 'html.parser')
        else:
            try:
                with open(file_path_to_use, 'r', encoding='utf-8') as f:
                    _ = f.read(2048)
                ext = '.html'
                image_data = extract_figures_from_html(file_path_to_use, output_dir, min_size=min_size)
                with open(file_path_to_use, 'r', encoding='utf-8') as f:
                    doc = BeautifulSoup(f, 'html.parser')
            except Exception:
                raise ValueError(f"Unsupported or unrecognized file type for: {file_path}")

        with open(output_file, "w", encoding="utf-8") as out:
            for img_path, img_bytes, img_ext, context_id in image_data:
                print(f"\nProcessing: {img_path}")
                context_text = get_context(doc, context_id, ext)
                description = gemini_describe(img_bytes, img_ext, context_text)
                out.write(f"{img_path}:\n{description}\n\n")

        if ext == '.pdf':
            doc.close()

        print(f"\n✅ All results saved to: {output_file}")
        print(f"✅ Extracted images saved under: {output_dir}")

    finally:
        if temp_file_to_remove and os.path.exists(temp_file_to_remove):
            try:
                os.remove(temp_file_to_remove)
            except Exception:
                pass


# ======================
# Run
# ======================
process_document(DOC_PATH)
