import fitz  # PyMuPDF
import os
import json
import psycopg2
import asyncio
import base64
from PIL import Image
import io

from dotenv import load_dotenv
load_dotenv()

import docx
from zipfile import ZipFile
from pptx import Presentation
from bs4 import BeautifulSoup
import requests
import tempfile
import shutil
import statistics
from urllib.parse import urlparse

# ======================
# Azure / DB integration (from your provided snippet)
# ======================
with open(f"{os.getcwd()}{os.sep}cred.json", "r") as f:
    cred = json.load(f)

def get_token_v2():
    con = psycopg2.connect(
        **cred["db"]
    )

    with con.cursor() as cur:
        spn = None
        try:
            cur.execute(
                f"Select value, last_update_time from meta where key='spn';"
            )

            results = cur.fetchall()
            if len(results) > 0:
                spn, last_update_time = results[0]
            else:
                raise Exception(f"Failed to select")
        except Exception as e:
            import traceback
            traceback.print_exc()

            raise Exception(f"{e}")

    return spn

token = get_token_v2()

# NOTE: keep the same class name you used earlier for your Azure client.
# It must be available in the runtime environment where you run this script.
# Example: from some_azure_wrapper import AsyncAzureOpenAI
# Here we assume AsyncAzureOpenAI is available in the environment.
from some_azure_wrapper import AsyncAzureOpenAI  # <-- ensure this import matches your environment

openai_llm = AsyncAzureOpenAI(
    azure_ad_token=token,
    azure_endpoint="https://iwpb-aap-dev-wus.openai.azure.com/",
    azure_deployment="gpt-4.1-nano",
    api_version="2025-01-01-preview"
)

# ======================
# CONFIG
# ======================
DOC_PATH = r"D:/Vrishabh/Personal Projects/PDF Extraction/test_docs/sample_test.pptx"
OUTPUT_FILE = "extracted_texts.txt"
MODEL = "gpt-4.1-nano"  # use deployment/model identifier appropriate for your Azure setup

# ======================
# FUNCTIONS (unchanged extractors & helpers from your original script)
# ======================

def extract_figures_from_pdf(pdf_path, output_dir, min_size):
    if not isinstance(min_size, int) or min_size < 0:
        raise ValueError("min_size must be a non-negative integer")

    os.makedirs(output_dir, exist_ok=True)
    doc = fitz.open(pdf_path)
    image_data = []
    skipped = 0

    for page_index, page in enumerate(doc):
        images = page.get_images(full=True)
        for img_index, img in enumerate(images):
            xref = img[0]
            smask = img[1]

            try:
                base_pix = fitz.Pixmap(doc, xref)
            except Exception:
                continue

            if getattr(base_pix, "alpha", False):
                try:
                    base_pix_no_alpha = fitz.Pixmap(base_pix, 0)
                    base_pix = base_pix_no_alpha
                except Exception:
                    pass

            pix = base_pix
            if smask and smask > 0:
                try:
                    mask_pix = fitz.Pixmap(doc, smask)
                    try:
                        pix = fitz.Pixmap(base_pix, mask_pix)
                    except Exception:
                        pix = base_pix
                    finally:
                        mask_pix = None
                except Exception:
                    pix = base_pix

            try:
                if getattr(pix, "colorspace", None) and pix.colorspace.n > 3:
                    pix = fitz.Pixmap(fitz.csRGB, pix)
            except Exception:
                pass

            try:
                sample_len = len(pix.samples)
            except Exception:
                sample_len = 0

            if sample_len < min_size:
                skipped += 1
                pix = None
                base_pix = None
                continue

            temp_path = os.path.join(output_dir, f"temp_page_{page_index+1}_figure_{img_index+1}.png")
            try:
                pix.save(temp_path)
            except Exception:
                pix = None
                base_pix = None
                try:
                    if os.path.exists(temp_path):
                        os.remove(temp_path)
                except Exception:
                    pass
                continue

            try:
                pil_img = Image.open(temp_path)
            except Exception:
                try:
                    os.remove(temp_path)
                except Exception:
                    pass
                pix = None
                base_pix = None
                continue

            if pil_img.mode in ('RGBA', 'LA', 'P'):
                if pil_img.mode == 'P':
                    pil_img = pil_img.convert('RGBA')
                background = Image.new("RGB", pil_img.size, (255, 255, 255))
                try:
                    alpha = pil_img.split()[3] if pil_img.mode == 'RGBA' else pil_img.split()[1]
                    background.paste(pil_img, mask=alpha)
                except Exception:
                    background.paste(pil_img)
                pil_img = background

            img_ext = "jpeg"
            img_path = os.path.join(output_dir, f"page_{page_index+1}_figure_{img_index+1}.{img_ext}")
            try:
                pil_img.save(img_path, "JPEG", quality=95)
            except Exception:
                try:
                    os.remove(temp_path)
                except Exception:
                    pass
                pix = None
                base_pix = None
                continue

            try:
                with open(img_path, "rb") as f:
                    img_bytes = f.read()
            except Exception:
                img_bytes = b''

            image_data.append((img_path, img_bytes, img_ext, page_index))

            try:
                os.remove(temp_path)
            except Exception:
                pass

            pix = None
            base_pix = None

    doc.close()
    print(f"✅ Extracted {len(image_data)} figure images to {output_dir} (skipped {skipped} by min_size)")
    return image_data


def extract_figures_from_docx(docx_path, output_dir, min_size):
    if not isinstance(min_size, int) or min_size < 0:
        raise ValueError("min_size must be a non-negative integer")

    os.makedirs(output_dir, exist_ok=True)
    image_data = []
    skipped = 0

    try:
        with ZipFile(docx_path) as zipf:
            media_files = [name for name in zipf.namelist() if name.startswith('word/media/')]
            for img_index, media in enumerate(media_files, 1):
                try:
                    img_bytes = zipf.read(media)
                except Exception:
                    continue
                if len(img_bytes) < min_size:
                    skipped += 1
                    continue
                try:
                    pil_img = Image.open(io.BytesIO(img_bytes))
                except Exception:
                    continue

                if pil_img.mode in ('RGBA', 'LA', 'P'):
                    if pil_img.mode == 'P':
                        pil_img = pil_img.convert('RGBA')
                    background = Image.new("RGB", pil_img.size, (255, 255, 255))
                    try:
                        alpha = pil_img.split()[3] if pil_img.mode == 'RGBA' else pil_img.split()[1]
                        background.paste(pil_img, mask=alpha)
                    except Exception:
                        background.paste(pil_img)
                    pil_img = background

                img_ext = "jpeg"
                base_name = os.path.splitext(os.path.basename(media))[0]
                img_path = os.path.join(output_dir, f"docx_{base_name}_{img_index}.{img_ext}")
                try:
                    pil_img.save(img_path, "JPEG", quality=95)
                except Exception:
                    continue

                try:
                    with open(img_path, "rb") as f:
                        saved_bytes = f.read()
                except Exception:
                    saved_bytes = img_bytes  # fallback

                image_data.append((img_path, saved_bytes, img_ext, 0))
    except Exception as e:
        print("⚠️ DOCX extraction error:", e)

    print(f"✅ Extracted {len(image_data)} figure images to {output_dir} (skipped {skipped} by min_size)")
    return image_data


def extract_figures_from_pptx(pptx_path, output_dir, min_size):
    if not isinstance(min_size, int) or min_size < 0:
        raise ValueError("min_size must be a non-negative integer")

    os.makedirs(output_dir, exist_ok=True)
    image_data = []
    skipped = 0

    try:
        with ZipFile(pptx_path) as zipf:
            media_files = [name for name in zipf.namelist() if name.startswith('ppt/media/')]
            for img_index, media in enumerate(media_files, 1):
                try:
                    img_bytes = zipf.read(media)
                except Exception:
                    continue
                if len(img_bytes) < min_size:
                    skipped += 1
                    continue
                try:
                    pil_img = Image.open(io.BytesIO(img_bytes))
                except Exception:
                    continue

                if pil_img.mode in ('RGBA', 'LA', 'P'):
                    if pil_img.mode == 'P':
                        pil_img = pil_img.convert('RGBA')
                    background = Image.new("RGB", pil_img.size, (255, 255, 255))
                    try:
                        alpha = pil_img.split()[3] if pil_img.mode == 'RGBA' else pil_img.split()[1]
                        background.paste(pil_img, mask=alpha)
                    except Exception:
                        background.paste(pil_img)
                    pil_img = background

                img_ext = "jpeg"
                base_name = os.path.splitext(os.path.basename(media))[0]
                img_path = os.path.join(output_dir, f"pptx_{base_name}_{img_index}.{img_ext}")
                try:
                    pil_img.save(img_path, "JPEG", quality=95)
                except Exception:
                    continue

                try:
                    with open(img_path, "rb") as f:
                        saved_bytes = f.read()
                except Exception:
                    saved_bytes = img_bytes

                image_data.append((img_path, saved_bytes, img_ext, 0))
    except Exception as e:
        print("⚠️ PPTX extraction error:", e)

    print(f"✅ Extracted {len(image_data)} figure images to {output_dir} (skipped {skipped} by min_size)")
    return image_data


def extract_figures_from_html(html_path, output_dir, min_size):
    if not isinstance(min_size, int) or min_size < 0:
        raise ValueError("min_size must be a non-negative integer")

    os.makedirs(output_dir, exist_ok=True)

    with open(html_path, 'r', encoding='utf-8') as f:
        soup = BeautifulSoup(f, 'html.parser')

    image_data = []
    base_dir = os.path.dirname(html_path)
    skipped = 0
    img_index = 0

    for img_tag in soup.find_all('img'):
        src = img_tag.get('src') or img_tag.get('data-src') or img_tag.get('data-original')
        if not src:
            continue

        if src.startswith('//'):
            src = 'https:' + src

        img_bytes = None
        if src.startswith('http://') or src.startswith('https://'):
            try:
                resp = requests.get(src, timeout=5)
                if resp.status_code == 200:
                    img_bytes = resp.content
            except Exception:
                img_bytes = None
        else:
            local_path = os.path.join(base_dir, src)
            if os.path.exists(local_path):
                try:
                    with open(local_path, 'rb') as lf:
                        img_bytes = lf.read()
                except Exception:
                    img_bytes = None

        if not img_bytes:
            continue

        if len(img_bytes) < min_size:
            skipped += 1
            continue

        try:
            pil_img = Image.open(io.BytesIO(img_bytes))
        except Exception:
            continue

        if pil_img.mode in ('RGBA', 'LA', 'P'):
            if pil_img.mode == 'P':
                pil_img = pil_img.convert('RGBA')
            background = Image.new("RGB", pil_img.size, (255, 255, 255))
            try:
                alpha = pil_img.split()[3] if pil_img.mode == 'RGBA' else pil_img.split()[1]
                background.paste(pil_img, mask=alpha)
            except Exception:
                background.paste(pil_img)
            pil_img = background

        img_ext = "jpeg"
        img_index += 1
        img_path = os.path.join(output_dir, f"html_image_{img_index}.{img_ext}")
        try:
            pil_img.save(img_path, "JPEG", quality=95)
        except Exception:
            continue

        try:
            with open(img_path, "rb") as f:
                saved_bytes = f.read()
        except Exception:
            saved_bytes = img_bytes

        image_data.append((img_path, saved_bytes, img_ext, 0))

    print(f"✅ Extracted {len(image_data)} figure images to {output_dir} (skipped {skipped} by min_size)")
    return image_data


def get_context(doc, context_id, ext):
    if ext == '.pdf':
        page = doc[context_id]
        return page.get_text()
    elif ext == '.docx':
        return '\n'.join([p.text for p in doc.paragraphs])
    elif ext == '.pptx':
        text = ''
        for slide in doc.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    text += shape.text + '\n'
        return text
    elif ext == '.html':
        return doc.get_text(separator='\n')
    else:
        return ''


# ======================
# Replaced Gemini -> Azure OpenAI integration
# ======================

def _build_prompt_for_image(page_text):
    prompt = f"Here is the text from the page containing this image:\n{page_text}\n\nExtract all visible text from the image and provide a detailed description of this image in the context of the document and the surrounding page text. If it's a diagram or chart, describe relationships, labels, structure, and what it illustrates in the context of the document."
    return prompt


async def _azure_generate_with_image_async(prompt_text, img_bytes, img_ext):
    """
    Adapter for AsyncAzureOpenAI from the official `openai` package.

    Strategy:
    - Build a messages list (chat-style) with the prompt as user content.
    - Attach the image as a base64 string appended to the prompt (many Azure/OpenAI
      wrappers accept only text-based input in chat endpoints).
    - Try common async client call shapes in this order:
        1) openai_llm.chat.completions.create(...)
        2) openai_llm.chat.create(...)              (alternate chat create name)
        3) create_chat_completion / get_chat_completions variants
        4) fallbacks: generate / complete
    - Normalize the response to return a plain text string.
    """
    import base64
    mime_type = f"image/{img_ext}"
    image_b64 = base64.b64encode(img_bytes).decode()

    # Build a chat-style messages list. We append the base64 as part of the user content.
    # This is compatible with the chat completion interfaces used by the OpenAI Python SDK.
    messages = [
        {"role": "user", "content": prompt_text + f"\n\n[IMAGE_BASE64:{image_b64}]"}
    ]

    # Candidate call attempts (ordered)
    last_exc = None

    # 1) Preferred: `client.chat.completions.create(...)` (common with the generated client)
    try:
        if hasattr(openai_llm, "chat") and hasattr(openai_llm.chat, "completions") and hasattr(openai_llm.chat.completions, "create"):
            resp = await openai_llm.chat.completions.create(model=MODEL, messages=messages)
            # Normalize and return
            # common shape: resp.choices[0].message.content or resp.choices[0].message["content"]
            if hasattr(resp, "choices") and len(resp.choices) > 0:
                first = resp.choices[0]
                # object style
                if hasattr(first, "message") and hasattr(first.message, "content"):
                    return first.message.content
                # dict-style inside
                if isinstance(first, dict):
                    msg = first.get("message") or first
                    if isinstance(msg, dict):
                        return msg.get("content") or msg.get("text") or str(resp)
                # fallback to text attribute
                if hasattr(first, "text"):
                    return first.text
            # some returns have .output or .text
            if hasattr(resp, "output"):
                return str(resp.output)
            if hasattr(resp, "text"):
                return str(resp.text)
            if isinstance(resp, dict):
                # common dict shape: {"choices":[{"message": {"content": "..."}}]}
                choices = resp.get("choices") or []
                if choices:
                    first = choices[0]
                    msg = first.get("message") or first
                    if isinstance(msg, dict):
                        return msg.get("content") or msg.get("text") or str(resp)
                # fallback fields
                return resp.get("value") or resp.get("content") or str(resp)
            return str(resp)
    except Exception as e:
        last_exc = e
        # If attribute doesn't exist, we'll try other shapes below. If call existed but failed (auth/network),
        # bubble up so you see the real error.
        if isinstance(e, AttributeError):
            pass
        else:
            raise

    # 2) Alternate chat create (some versions / wrappers expose chat.create)
    try:
        if hasattr(openai_llm, "chat") and hasattr(openai_llm.chat, "create"):
            resp = await openai_llm.chat.create(model=MODEL, messages=messages)
            # Try to extract content similarly
            if isinstance(resp, dict):
                choices = resp.get("choices") or []
                if choices:
                    first = choices[0]
                    if isinstance(first, dict):
                        msg = first.get("message") or first
                        if isinstance(msg, dict):
                            return msg.get("content") or msg.get("text") or str(resp)
                    return str(first)
            if hasattr(resp, "choices") and len(resp.choices) > 0:
                first = resp.choices[0]
                if hasattr(first, "message") and hasattr(first.message, "content"):
                    return first.message.content
            if hasattr(resp, "text"):
                return resp.text
            return str(resp)
    except Exception as e:
        last_exc = e
        if not isinstance(e, AttributeError):
            raise

    # 3) Other common SDK convenience methods
    candidates = [
        ("create_chat_completion", lambda: openai_llm.create_chat_completion(model=MODEL, messages=messages)),
        ("get_chat_completions", lambda: openai_llm.get_chat_completions(model=MODEL, messages=messages)),
        ("get_chat_completion", lambda: openai_llm.get_chat_completion(model=MODEL, messages=messages)),
        ("generate", lambda: openai_llm.generate({"model": MODEL, "messages": messages})),
        ("complete", lambda: openai_llm.complete({"model": MODEL, "messages": messages})),
    ]
    for name, fn in candidates:
        try:
            if hasattr(openai_llm, name) or name in ("generate", "complete"):
                resp = await fn()
                # Normalization similar to above
                if hasattr(resp, "text"):
                    return resp.text
                if isinstance(resp, dict):
                    choices = resp.get("choices") or []
                    if choices:
                        first = choices[0]
                        if isinstance(first, dict):
                            msg = first.get("message") or first
                            if isinstance(msg, dict):
                                return msg.get("content") or msg.get("text") or str(resp)
                            return str(first)
                    if "value" in resp:
                        return resp["value"]
                    if "content" in resp:
                        return resp["content"]
                    return str(resp)
                if hasattr(resp, "choices") and len(resp.choices) > 0:
                    first = resp.choices[0]
                    if hasattr(first, "message") and hasattr(first.message, "content"):
                        return first.message.content
                    if hasattr(first, "text"):
                        return first.text
                    return str(first)
                return str(resp)
        except AttributeError as e:
            last_exc = e
            continue
        except Exception:
            # If method exists but failed (auth, request format), raise to surface the actual error.
            raise

    # If no method existed or succeeded, raise a clear message including the last AttributeError
    raise AttributeError(
        "openai_llm (AsyncAzureOpenAI) does not expose a known async chat/complete method on this SDK version. "
        "Tried `chat.completions.create`, `chat.create`, and common alternatives. "
        f"Last error: {last_exc}"
    )



def gemini_describe(img_bytes, img_ext, page_text):
    """Replaced Gemini call with Azure OpenAI async call (synchronous wrapper)."""
    prompt = _build_prompt_for_image(page_text)
    # Run the async helper synchronously
    try:
        result = asyncio.run(_azure_generate_with_image_async(prompt, img_bytes, img_ext))
    except Exception as e:
        # If async run fails, raise to surface issues like incorrect client API usage.
        raise

    return result.strip() if result else ""


# ======================
# The rest of the original script: helpers and process_document
# ======================

def _download_url_to_tempfile(url, timeout=10):
    try:
        head = requests.head(url, allow_redirects=True, timeout=5)
    except Exception:
        head = None

    content_type = None
    if head is not None and 'content-type' in head.headers:
        content_type = head.headers['content-type']

    if not content_type:
        resp = requests.get(url, stream=True, timeout=timeout)
    else:
        resp = requests.get(url, stream=True, timeout=timeout)

    if resp.status_code >= 400:
        raise ValueError(f"Failed to fetch URL: {url} (status {resp.status_code})")

    ct = resp.headers.get('content-type', content_type or '').lower()

    if 'html' in ct:
        ext = '.html'
    elif 'pdf' in ct:
        ext = '.pdf'
    elif 'presentation' in ct or 'pptx' in ct:
        ext = '.pptx'
    elif 'word' in ct or 'officedocument.wordprocessingml.document' in ct or 'msword' in ct:
        ext = '.docx'
    elif 'image' in ct:
        subtype = ct.split('/')[-1].split(';')[0] if '/' in ct else 'jpeg'
        ext = f".{subtype}" if not subtype.startswith('x-') else f".{subtype.replace('x-', '')}"
    else:
        path = urlparse(url).path
        _, ext = os.path.splitext(path)
        if not ext:
            ext = '.html'

    tmp_fd, tmp_path = tempfile.mkstemp(suffix=ext)
    os.close(tmp_fd)

    with open(tmp_path, 'wb') as out_f:
        for chunk in resp.iter_content(chunk_size=8192):
            if chunk:
                out_f.write(chunk)

    return tmp_path, ext


def determine_min_size(file_path, ext, mode='balanced', sample_limit=8, min_absolute=5000):
    sizes = []
    try:
        if ext == '.pdf':
            doc = fitz.open(file_path)
            for page_index, page in enumerate(doc):
                if len(sizes) >= sample_limit:
                    break
                for img in page.get_images(full=True):
                    if len(sizes) >= sample_limit:
                        break
                    xref = img[0]
                    try:
                        pix = fitz.Pixmap(doc, xref)
                        sizes.append(len(pix.samples))
                        pix = None
                    except Exception:
                        continue
            doc.close()

        elif ext == '.docx':
            try:
                doc = docx.Document(file_path)
                for rel in list(doc.part.rels.values()):
                    if len(sizes) >= sample_limit:
                        break
                    if "image" in rel.reltype:
                        try:
                            blob = rel.target_part.blob
                            sizes.append(len(blob))
                        except Exception:
                            continue
            except Exception:
                pass

        elif ext == '.pptx':
            try:
                with ZipFile(file_path) as zipf:
                    media_files = [name for name in zipf.namelist() if name.startswith('ppt/media/')]
                    for media in media_files[:sample_limit]:
                        try:
                            b = zipf.read(media)
                            sizes.append(len(b))
                        except Exception:
                            continue
            except Exception:
                pass

        elif ext in ('.html', '.htm'):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    soup = BeautifulSoup(f, 'html.parser')
                img_tags = soup.find_all('img')[:sample_limit]
                for tag in img_tags:
                    src = (tag.get('src') or tag.get('data-src') or tag.get('data-original') or tag.get('data-srcset') or '').strip()
                    if not src:
                        continue
                    if src.startswith('//'):
                        src = 'https:' + src
                    if src.startswith('http'):
                        try:
                            head = requests.head(src, allow_redirects=True, timeout=3)
                            cl = head.headers.get('content-length')
                            if cl and cl.isdigit():
                                sizes.append(int(cl))
                                continue
                        except Exception:
                            pass
                        try:
                            resp = requests.get(src, stream=True, timeout=5)
                            if resp.status_code == 200:
                                sizes.append(len(resp.content))
                        except Exception:
                            continue
                    else:
                        local_path = os.path.join(os.path.dirname(file_path), src)
                        if os.path.exists(local_path):
                            try:
                                with open(local_path, 'rb') as lf:
                                    data = lf.read()
                                    sizes.append(len(data))
                            except Exception:
                                continue
            except Exception:
                pass

    except Exception:
        sizes = []

    if mode == 'aggressive':
        multiplier = 0.25
    elif mode == 'conservative':
        multiplier = 1.2
    else:
        multiplier = 0.5

    if sizes:
        med = int(statistics.median(sizes))
        computed = max(int(med * multiplier), min_absolute)
        computed = int(max(min_absolute, min(computed, max(200000, med * 4))))
        return computed
    else:
        fallback_map = {
            '.pdf': 50000,
            '.docx': 15000,
            '.pptx': 20000,
            '.html': 8000
        }
        return fallback_map.get(ext, min_absolute)


def process_document(file_path, determine_mode='balanced'):
    temp_file_to_remove = None
    is_url = isinstance(file_path, str) and file_path.lower().startswith(('http://', 'https://'))
    original_url = file_path if is_url else None

    if is_url:
        print(f"Detected URL. Downloading: {file_path}")
        try:
            tmp_path, tmp_ext = _download_url_to_tempfile(file_path)
        except Exception as e:
            raise ValueError(f"Failed to download URL: {file_path}. Error: {e}")
        temp_file_to_remove = tmp_path
        file_path_to_use = tmp_path
    else:
        file_path_to_use = file_path

    try:
        ext = os.path.splitext(file_path_to_use)[1].lower()

        base_output_dir = f"extracted_images_{ext[1:] if ext else 'unknown'}"

        if is_url:
            parsed = urlparse(file_path)
            url_basename = os.path.basename(parsed.path) or parsed.netloc
            file_folder = os.path.splitext(url_basename)[0]
            file_folder = "".join(c for c in file_folder if c.isalnum() or c in ('_', '-')).strip() or "downloaded_file"
        else:
            file_folder = os.path.splitext(os.path.basename(file_path_to_use))[0]

        output_dir = os.path.join(base_output_dir, file_folder)
        os.makedirs(output_dir, exist_ok=True)

        output_file = os.path.join(output_dir, f"extracted_texts_{file_folder}.txt")

        min_size = determine_min_size(file_path_to_use, ext, mode=determine_mode, sample_limit=8, min_absolute=5000)
        print(f"Using min_size={min_size} bytes for extraction (mode={determine_mode}).")

        if ext == '.pdf':
            image_data = extract_figures_from_pdf(file_path_to_use, output_dir, min_size=min_size)
            doc = fitz.open(file_path_to_use)
        elif ext == '.docx':
            image_data = extract_figures_from_docx(file_path_to_use, output_dir, min_size=min_size)
            doc = docx.Document(file_path_to_use)
        elif ext == '.pptx':
            image_data = extract_figures_from_pptx(file_path_to_use, output_dir, min_size=min_size)
            doc = Presentation(file_path_to_use)
        elif ext in ('.html', '.htm'):
            image_data = extract_figures_from_html(file_path_to_use, output_dir, min_size=min_size)
            with open(file_path_to_use, 'r', encoding='utf-8') as f:
                doc = BeautifulSoup(f, 'html.parser')
        else:
            try:
                with open(file_path_to_use, 'r', encoding='utf-8') as f:
                    _ = f.read(2048)
                ext = '.html'
                image_data = extract_figures_from_html(file_path_to_use, output_dir, min_size=min_size)
                with open(file_path_to_use, 'r', encoding='utf-8') as f:
                    doc = BeautifulSoup(f, 'html.parser')
            except Exception:
                raise ValueError(f"Unsupported or unrecognized file type for: {file_path}")

        with open(output_file, "w", encoding="utf-8") as out:
            for img_path, img_bytes, img_ext, context_id in image_data:
                print(f"\nProcessing: {img_path}")
                context_text = get_context(doc, context_id, ext)
                description = gemini_describe(img_bytes, img_ext, context_text)
                out.write(f"{img_path}:\n{description}\n\n")

        if ext == '.pdf':
            doc.close()

        print(f"\n✅ All results saved to: {output_file}")
        print(f"✅ Extracted images saved under: {output_dir}")

    finally:
        if temp_file_to_remove and os.path.exists(temp_file_to_remove):
            try:
                os.remove(temp_file_to_remove)
            except Exception:
                pass


# ======================
# Run
# ======================
process_document(DOC_PATH)
