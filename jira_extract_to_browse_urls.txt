# jira_extract_to_browse_urls_cli.py
"""
Usage:
  - Place next to internal.py (which must define class Jira with method get_ori(key))
  - pip install beautifulsoup4
  - Run:
      python jira_extract_to_browse_urls_cli.py https://.../browse/ABC-1 https://.../browse/DEF-2
    or
      python jira_extract_to_browse_urls_cli.py --file my_urls.txt
Output:
  - jira_issue_urls.json  (mapping: browse_url -> [urls...])
  - debug files for non-JSON responses: {KEY}_raw_response.txt
"""

import sys
import argparse
import re
import json
from pathlib import Path
from typing import Any, Set
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup

from internal import Jira  # your Jira class

URL_REGEX = re.compile(r'https?://[^\s)>\]\["\']+')
MD_LINK_REGEX = re.compile(r'\[.*?\]\((https?://[^\s)]+)\)')
ANGLE_LINK_REGEX = re.compile(r'<(https?://[^>]+)>')

def ensure_abs(href: str, host_base: str) -> str:
    if not href:
        return href
    parsed = urlparse(href)
    if parsed.scheme:
        return href
    return urljoin(host_base, href)

def extract_urls_from_html(html: str) -> Set[str]:
    found = set()
    if not html:
        return found
    try:
        soup = BeautifulSoup(html, "html.parser")
    except Exception:
        soup = None

    if soup:
        for a in soup.find_all("a", href=True):
            href = a["href"].strip()
            if href:
                found.add(href)
        for tag in soup.find_all(True):
            for attr in ("src", "data-src", "data-href", "content"):
                if tag.has_attr(attr):
                    val = tag.get(attr)
                    if isinstance(val, str) and val.startswith("http"):
                        found.add(val.strip())

    # regex fallback
    for u in URL_REGEX.findall(html):
        found.add(u.strip())
    for u in MD_LINK_REGEX.findall(html):
        found.add(u.strip())
    for u in ANGLE_LINK_REGEX.findall(html):
        found.add(u.strip())
    return found

def extract_urls_from_text(text: str) -> Set[str]:
    found = set()
    if not text:
        return found
    for u in URL_REGEX.findall(text):
        found.add(u.strip())
    for u in MD_LINK_REGEX.findall(text):
        found.add(u.strip())
    for u in ANGLE_LINK_REGEX.findall(text):
        found.add(u.strip())
    return found

def scan_attachment_for_urls(att: dict) -> Set[str]:
    urls = set()
    if not isinstance(att, dict):
        return urls
    for k in ("content", "self", "thumbnail", "url", "download"):
        v = att.get(k)
        if isinstance(v, str) and v.startswith("http"):
            urls.add(v.strip())
    for v in att.values():
        if isinstance(v, str):
            urls.update(extract_urls_from_text(v))
        elif isinstance(v, (dict, list)):
            urls.update(recursive_scan_for_urls(v))
    return urls

def recursive_scan_for_urls(obj: Any) -> Set[str]:
    found = set()
    if obj is None:
        return found
    if isinstance(obj, str):
        found.update(extract_urls_from_html(obj))
        found.update(extract_urls_from_text(obj))
        return found
    if isinstance(obj, dict):
        for k, v in obj.items():
            if k.lower() in ("attachment", "attachments") and isinstance(v, list):
                for att in v:
                    found.update(scan_attachment_for_urls(att))
            else:
                found.update(recursive_scan_for_urls(v))
        return found
    if isinstance(obj, list):
        for itm in obj:
            found.update(recursive_scan_for_urls(itm))
        return found
    return found

def filter_out_internal_jira_urls(urls: Set[str], host_base: str) -> Set[str]:
    clean = set()
    rest_indicator = f"{host_base}/rest/api"
    for u in urls:
        if not isinstance(u, str) or not u.strip():
            continue
        u = u.strip()
        # make absolute if relative
        if u.startswith("/"):
            u = urljoin(host_base, u)
        # Skip generic JIRA REST noise unless it's an attachment
        if rest_indicator in u:
            if "/attachment/" in u or "/secure/attachment/" in u:
                clean.add(u)
            else:
                continue
        else:
            clean.add(u)
    return clean

def extract_issue_key_from_browse_url(browse_url: str) -> str:
    return browse_url.rstrip("/").split("/")[-1]

def host_base_from_browse_url(browse_url: str) -> str:
    p = urlparse(browse_url)
    return f"{p.scheme}://{p.netloc}"

def process_single(jira: Jira, browse_url: str):
    key = extract_issue_key_from_browse_url(browse_url)
    print(f"Processing {key} ...")
    raw = jira.get_ori(key)

    # handle text or dict responses
    if isinstance(raw, str):
        try:
            resp = json.loads(raw)
        except Exception:
            Path(f"{key}_raw_response.txt").write_text(raw, encoding="utf-8")
            print(f"Non-JSON response saved to {key}_raw_response.txt")
            return []
        else:
            resp_obj = resp
    elif isinstance(raw, dict):
        resp_obj = raw
    else:
        print(f"Unexpected response type for {key}: {type(raw)}")
        return []

    # if wrapper contains issues[]
    if isinstance(resp_obj, dict) and resp_obj.get("issues"):
        issue = resp_obj["issues"][0]
    else:
        issue = resp_obj

    all_urls = set()
    host_base = host_base_from_browse_url(browse_url)

    # renderedFields preferred
    rendered = issue.get("renderedFields") or {}
    if rendered:
        all_urls.update(extract_urls_from_html(rendered.get("description", "")))
        comm_block = rendered.get("comment") or {}
        if isinstance(comm_block, dict):
            for c in comm_block.get("comments", []):
                body = c.get("body") or c.get("renderedBody") or ""
                all_urls.update(extract_urls_from_html(body))

    # raw fields
    fields = issue.get("fields") or {}
    all_urls.update(extract_urls_from_html(fields.get("description", "")))
    comments = fields.get("comment", {}).get("comments", []) if isinstance(fields.get("comment", {}), dict) else []
    for c in comments:
        body = c.get("body", "")
        all_urls.update(extract_urls_from_html(body))
        all_urls.update(extract_urls_from_text(body))

    # attachments
    for att in fields.get("attachment", []) if isinstance(fields.get("attachment", []), list) else []:
        all_urls.update(scan_attachment_for_urls(att))

    # fallback: scan everything
    all_urls.update(recursive_scan_for_urls(issue))

    # normalize and filter
    normalized = set()
    for u in all_urls:
        if not isinstance(u, str) or not u.strip():
            continue
        if u.startswith("/"):
            u = urljoin(host_base, u)
        normalized.add(u.strip())

    filtered = filter_out_internal_jira_urls(normalized, host_base)
    return sorted(filtered)

def parse_args():
    p = argparse.ArgumentParser(description="Extract URLs from JIRA browse URLs using internal.Jira.get_ori")
    p.add_argument("browse_urls", nargs="*", help="one or more jira browse URLs")
    p.add_argument("--file", "-f", help="newline-separated file with browse URLs")
    p.add_argument("--out", "-o", default="jira_issue_urls.json", help="output JSON filename")
    return p.parse_args()

def main():
    args = parse_args()
    urls = []
    if args.file:
        path = Path(args.file)
        if not path.exists():
            print(f"File not found: {args.file}")
            return
        urls = [line.strip() for line in path.read_text(encoding="utf-8").splitlines() if line.strip()]
    if args.browse_urls:
        urls.extend(args.browse_urls)
    if not urls:
        print("No browse URLs provided. Provide as CLI args or via --file urls.txt")
        return

    jira = Jira()
    out = {}
    for b in urls:
        try:
            res = process_single(jira, b)
        except Exception as e:
            print(f"Error processing {b}: {e}")
            res = []
        out[b] = res
        print(f" -> {len(res)} urls found for {b}")

    Path(args.out).write_text(json.dumps(out, indent=2, ensure_ascii=False), encoding="utf-8")
    print(f"Saved results to {args.out}")

if __name__ == "__main__":
    main()
