import fitz  # PyMuPDF
import os
from google import genai
from google.genai import types
from PIL import Image
import io

from dotenv import load_dotenv
load_dotenv()

import docx
from zipfile import ZipFile
from pptx import Presentation
from bs4 import BeautifulSoup
import requests
import tempfile
import shutil
import statistics
from urllib.parse import urlparse

# ======================
# CONFIG
# ======================
DOC_PATH = r"sample_test.pdf"
OUTPUT_FILE = "extracted_texts.txt"
MODEL = "gemini-2.0-flash"

client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))  # Replace with your API key

# PAPER_CONTEXT = """
# This image is from the arXiv paper 2409.13731v3: "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation" by Lei Liang.

# Abstract: The recently developed retrieval-augmented generation (RAG) technology has enabled the efficient construction of domain-specific applications. However, it also has limitations, including the gap between vector similarity and the relevance of knowledge reasoning, as well as insensitivity to knowledge logic, such as numerical values, temporal relations, expert rules, and others, which hinder the effectiveness of professional knowledge services. In this work, we introduce a professional domain knowledge service framework called Knowledge Augmented Generation (KAG). KAG is designed to address the aforementioned challenges with the motivation of making full use of the advantages of knowledge graph(KG) and vector retrieval, and to improve generation and reasoning performance by bidirectionally enhancing large language models (LLMs) and KGs through five key aspects: (1) LLM-friendly knowledge representation, (2) mutual-indexing between knowledge graphs and original chunks, (3) logical-form-guided hybrid reasoning engine, (4) knowledge alignment with semantic reasoning, and (5) model capability enhancement for KAG. We compared KAG with existing RAG methods in multihop question answering and found that it significantly outperforms state-of-the-art methods, achieving a relative improvement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We have successfully applied KAG to two professional knowledge Q&A tasks of Ant Group, including E-Government Q&A and E-Health Q&A, achieving significant improvement in professionalism compared to RAG methods.
# """

# ======================
# FUNCTIONS
# ======================

def extract_figures_from_pdf(pdf_path, output_dir, min_size):
    """
    Extract raster images from a PDF (page-level images).
    - pdf_path: local path to PDF
    - output_dir: folder to write normalized JPEGs
    - min_size: integer threshold (bytes) measured on fitz.Pixmap.samples (uncompressed samples)
    Returns: list of tuples (img_path, img_bytes, img_ext, page_index)
    """
    if not isinstance(min_size, int) or min_size < 0:
        raise ValueError("min_size must be a non-negative integer")

    os.makedirs(output_dir, exist_ok=True)
    doc = fitz.open(pdf_path)
    image_data = []
    skipped = 0

    for page_index, page in enumerate(doc):
        images = page.get_images(full=True)
        for img_index, img in enumerate(images):
            xref = img[0]
            smask = img[1]

            try:
                base_pix = fitz.Pixmap(doc, xref)
            except Exception:
                # Unable to construct pixmap for this xref
                continue

            # try to handle alpha / soft mask
            if getattr(base_pix, "alpha", False):
                try:
                    base_pix_no_alpha = fitz.Pixmap(base_pix, 0)
                    base_pix = base_pix_no_alpha
                except Exception:
                    pass

            pix = base_pix
            if smask and smask > 0:
                try:
                    mask_pix = fitz.Pixmap(doc, smask)
                    try:
                        pix = fitz.Pixmap(base_pix, mask_pix)
                    except Exception:
                        pix = base_pix
                    finally:
                        mask_pix = None
                except Exception:
                    pix = base_pix

            # Convert to RGB if necessary (e.g., CMYK)
            try:
                if getattr(pix, "colorspace", None) and pix.colorspace.n > 3:
                    pix = fitz.Pixmap(fitz.csRGB, pix)
            except Exception:
                pass

            # Size filter using uncompressed samples (consistent with earlier approach)
            try:
                sample_len = len(pix.samples)
            except Exception:
                sample_len = 0

            if sample_len < min_size:
                skipped += 1
                # release pix resources
                pix = None
                base_pix = None
                continue

            # Save temporarily as PNG, then open with PIL to flatten transparency
            temp_path = os.path.join(output_dir, f"temp_page_{page_index+1}_figure_{img_index+1}.png")
            try:
                pix.save(temp_path)
            except Exception:
                pix = None
                base_pix = None
                try:
                    if os.path.exists(temp_path):
                        os.remove(temp_path)
                except Exception:
                    pass
                continue

            try:
                pil_img = Image.open(temp_path)
            except Exception:
                try:
                    os.remove(temp_path)
                except Exception:
                    pass
                pix = None
                base_pix = None
                continue

            # Flatten transparency to white
            if pil_img.mode in ('RGBA', 'LA', 'P'):
                if pil_img.mode == 'P':
                    pil_img = pil_img.convert('RGBA')
                background = Image.new("RGB", pil_img.size, (255, 255, 255))
                # attempt to paste using alpha channel if present
                try:
                    alpha = pil_img.split()[3] if pil_img.mode == 'RGBA' else pil_img.split()[1]
                    background.paste(pil_img, mask=alpha)
                except Exception:
                    background.paste(pil_img)
                pil_img = background

            img_ext = "jpeg"
            img_path = os.path.join(output_dir, f"page_{page_index+1}_figure_{img_index+1}.{img_ext}")
            try:
                pil_img.save(img_path, "JPEG", quality=95)
            except Exception:
                try:
                    os.remove(temp_path)
                except Exception:
                    pass
                pix = None
                base_pix = None
                continue

            try:
                with open(img_path, "rb") as f:
                    img_bytes = f.read()
            except Exception:
                img_bytes = b''

            image_data.append((img_path, img_bytes, img_ext, page_index))

            # cleanup
            try:
                os.remove(temp_path)
            except Exception:
                pass

            pix = None
            base_pix = None

    doc.close()
    print(f"✅ Extracted {len(image_data)} figure images to {output_dir} (skipped {skipped} by min_size)")
    return image_data


def extract_figures_from_docx(docx_path, output_dir, min_size):
    """
    Extract images from a .docx by reading word/media/* entries from the zip.
    - min_size is required and applied to raw blob length (bytes).
    Returns list of (img_path, img_bytes, img_ext, 0)
    """
    if not isinstance(min_size, int) or min_size < 0:
        raise ValueError("min_size must be a non-negative integer")

    os.makedirs(output_dir, exist_ok=True)
    image_data = []
    skipped = 0

    try:
        with ZipFile(docx_path) as zipf:
            media_files = [name for name in zipf.namelist() if name.startswith('word/media/')]
            for img_index, media in enumerate(media_files, 1):
                try:
                    img_bytes = zipf.read(media)
                except Exception:
                    continue
                if len(img_bytes) < min_size:
                    skipped += 1
                    continue
                try:
                    pil_img = Image.open(io.BytesIO(img_bytes))
                except Exception:
                    continue

                if pil_img.mode in ('RGBA', 'LA', 'P'):
                    if pil_img.mode == 'P':
                        pil_img = pil_img.convert('RGBA')
                    background = Image.new("RGB", pil_img.size, (255, 255, 255))
                    try:
                        alpha = pil_img.split()[3] if pil_img.mode == 'RGBA' else pil_img.split()[1]
                        background.paste(pil_img, mask=alpha)
                    except Exception:
                        background.paste(pil_img)
                    pil_img = background

                img_ext = "jpeg"
                base_name = os.path.splitext(os.path.basename(media))[0]
                img_path = os.path.join(output_dir, f"docx_{base_name}_{img_index}.{img_ext}")
                try:
                    pil_img.save(img_path, "JPEG", quality=95)
                except Exception:
                    continue

                try:
                    with open(img_path, "rb") as f:
                        saved_bytes = f.read()
                except Exception:
                    saved_bytes = img_bytes  # fallback

                image_data.append((img_path, saved_bytes, img_ext, 0))
    except Exception as e:
        print("⚠️ DOCX extraction error:", e)

    print(f"✅ Extracted {len(image_data)} figure images to {output_dir} (skipped {skipped} by min_size)")
    return image_data


def extract_figures_from_pptx(pptx_path, output_dir, min_size):
    """
    Extract images from a .pptx ZIP (ppt/media/*).
    - min_size is required and applied to raw media blob length (bytes).
    Returns list of (img_path, img_bytes, img_ext, 0)
    """
    if not isinstance(min_size, int) or min_size < 0:
        raise ValueError("min_size must be a non-negative integer")

    os.makedirs(output_dir, exist_ok=True)
    image_data = []
    skipped = 0

    try:
        with ZipFile(pptx_path) as zipf:
            media_files = [name for name in zipf.namelist() if name.startswith('ppt/media/')]
            for img_index, media in enumerate(media_files, 1):
                try:
                    img_bytes = zipf.read(media)
                except Exception:
                    continue
                if len(img_bytes) < min_size:
                    skipped += 1
                    continue
                try:
                    pil_img = Image.open(io.BytesIO(img_bytes))
                except Exception:
                    continue

                if pil_img.mode in ('RGBA', 'LA', 'P'):
                    if pil_img.mode == 'P':
                        pil_img = pil_img.convert('RGBA')
                    background = Image.new("RGB", pil_img.size, (255, 255, 255))
                    try:
                        alpha = pil_img.split()[3] if pil_img.mode == 'RGBA' else pil_img.split()[1]
                        background.paste(pil_img, mask=alpha)
                    except Exception:
                        background.paste(pil_img)
                    pil_img = background

                img_ext = "jpeg"
                base_name = os.path.splitext(os.path.basename(media))[0]
                img_path = os.path.join(output_dir, f"pptx_{base_name}_{img_index}.{img_ext}")
                try:
                    pil_img.save(img_path, "JPEG", quality=95)
                except Exception:
                    continue

                try:
                    with open(img_path, "rb") as f:
                        saved_bytes = f.read()
                except Exception:
                    saved_bytes = img_bytes

                image_data.append((img_path, saved_bytes, img_ext, 0))
    except Exception as e:
        print("⚠️ PPTX extraction error:", e)

    print(f"✅ Extracted {len(image_data)} figure images to {output_dir} (skipped {skipped} by min_size)")
    return image_data


def extract_figures_from_html(html_path, output_dir, min_size):
    """
    Extract images referenced in an HTML file. Supports http/https and local src paths.
    - min_size is required and applied to fetched image bytes length.
    Returns list of (img_path, img_bytes, img_ext, 0)
    """
    if not isinstance(min_size, int) or min_size < 0:
        raise ValueError("min_size must be a non-negative integer")

    os.makedirs(output_dir, exist_ok=True)

    # Read HTML content
    with open(html_path, 'r', encoding='utf-8') as f:
        soup = BeautifulSoup(f, 'html.parser')

    image_data = []
    base_dir = os.path.dirname(html_path)
    skipped = 0
    img_index = 0

    for img_tag in soup.find_all('img'):
        src = img_tag.get('src') or img_tag.get('data-src') or img_tag.get('data-original')
        if not src:
            continue

        # Resolve protocol-relative URLs
        if src.startswith('//'):
            src = 'https:' + src

        # Fetch bytes (remote or local)
        img_bytes = None
        if src.startswith('http://') or src.startswith('https://'):
            try:
                resp = requests.get(src, timeout=5)
                if resp.status_code == 200:
                    img_bytes = resp.content
            except Exception:
                img_bytes = None
        else:
            local_path = os.path.join(base_dir, src)
            if os.path.exists(local_path):
                try:
                    with open(local_path, 'rb') as lf:
                        img_bytes = lf.read()
                except Exception:
                    img_bytes = None

        if not img_bytes:
            continue

        if len(img_bytes) < min_size:
            skipped += 1
            continue

        try:
            pil_img = Image.open(io.BytesIO(img_bytes))
        except Exception:
            continue

        if pil_img.mode in ('RGBA', 'LA', 'P'):
            if pil_img.mode == 'P':
                pil_img = pil_img.convert('RGBA')
            background = Image.new("RGB", pil_img.size, (255, 255, 255))
            try:
                alpha = pil_img.split()[3] if pil_img.mode == 'RGBA' else pil_img.split()[1]
                background.paste(pil_img, mask=alpha)
            except Exception:
                background.paste(pil_img)
            pil_img = background

        img_ext = "jpeg"
        img_index += 1
        img_path = os.path.join(output_dir, f"html_image_{img_index}.{img_ext}")
        try:
            pil_img.save(img_path, "JPEG", quality=95)
        except Exception:
            continue

        try:
            with open(img_path, "rb") as f:
                saved_bytes = f.read()
        except Exception:
            saved_bytes = img_bytes

        image_data.append((img_path, saved_bytes, img_ext, 0))

    print(f"✅ Extracted {len(image_data)} figure images to {output_dir} (skipped {skipped} by min_size)")
    return image_data


def get_context(doc, context_id, ext):
    if ext == '.pdf':
        page = doc[context_id]
        return page.get_text()
    elif ext == '.docx':
        return '\n'.join([p.text for p in doc.paragraphs])
    elif ext == '.pptx':
        text = ''
        for slide in doc.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    text += shape.text + '\n'
        return text
    elif ext == '.html':
        return doc.get_text(separator='\n')
    else:
        return ''


def gemini_describe(img_bytes, img_ext, page_text):
    """Use Gemini to describe image content in detail with contextual understanding, using the original image bytes."""
    mime_type = f"image/{img_ext}"

    prompt = f"Here is the text from the page containing this image:\n{page_text}\n\nExtract all visible text from the image and provide a detailed description of this image in the context of the document and the surrounding page text. If it's a diagram or chart, describe relationships, labels, structure, and what it illustrates in the context of the document."

    response = client.models.generate_content(
        model=MODEL,
        contents=[
            types.Part(text=prompt),
            types.Part(inline_data=types.Blob(
                mime_type=mime_type,
                data=img_bytes
            ))
        ]
    )
    return response.text.strip()


def _download_url_to_tempfile(url, timeout=10):
    """
    Download a URL and save to a temporary file. Returns (temp_path, guessed_ext).
    Guesses extension from Content-Type first, then from URL path.
    Caller is responsible for removing temp_path when done.
    """
    try:
        # Try a HEAD first to get content-type without downloading body (some servers may not support HEAD)
        head = requests.head(url, allow_redirects=True, timeout=5)
    except Exception:
        head = None

    content_type = None
    if head is not None and 'content-type' in head.headers:
        content_type = head.headers['content-type']

    # If HEAD didn't give us a content-type, do a GET (we will save the content anyway)
    if not content_type:
        resp = requests.get(url, stream=True, timeout=timeout)
    else:
        # If HEAD gave content-type, still do a GET to fetch content for saving
        resp = requests.get(url, stream=True, timeout=timeout)

    if resp.status_code >= 400:
        raise ValueError(f"Failed to fetch URL: {url} (status {resp.status_code})")

    ct = resp.headers.get('content-type', content_type or '').lower()

    # Map common content-types to extensions
    if 'html' in ct:
        ext = '.html'
    elif 'pdf' in ct:
        ext = '.pdf'
    elif 'presentation' in ct or 'pptx' in ct:
        ext = '.pptx'
    elif 'word' in ct or 'officedocument.wordprocessingml.document' in ct or 'msword' in ct:
        ext = '.docx'
    elif 'image' in ct:
        # If direct image link, use the image extension from content-type or url
        subtype = ct.split('/')[-1].split(';')[0] if '/' in ct else 'jpeg'
        ext = f".{subtype}" if not subtype.startswith('x-') else f".{subtype.replace('x-', '')}"
    else:
        # fallback: try to get extension from URL path
        path = urlparse(url).path
        _, ext = os.path.splitext(path)
        if not ext:
            # fallback to html
            ext = '.html'

    # Create a temp file
    tmp_fd, tmp_path = tempfile.mkstemp(suffix=ext)
    os.close(tmp_fd)

    # Stream to file
    with open(tmp_path, 'wb') as out_f:
        for chunk in resp.iter_content(chunk_size=8192):
            if chunk:
                out_f.write(chunk)

    return tmp_path, ext


def determine_min_size(file_path, ext, mode='balanced', sample_limit=8, min_absolute=5000):
    """
    Adaptive threshold for filtering small images.
    Returns an integer min_size (bytes) computed by sampling a few images from the document.
    - mode: 'aggressive' (low threshold), 'balanced' (default), 'conservative' (higher threshold)
    - sample_limit: max number of images to sample
    - min_absolute: absolute floor for the threshold
    """
    sizes = []
    try:
        if ext == '.pdf':
            doc = fitz.open(file_path)
            for page_index, page in enumerate(doc):
                if len(sizes) >= sample_limit:
                    break
                for img in page.get_images(full=True):
                    if len(sizes) >= sample_limit:
                        break
                    xref = img[0]
                    try:
                        pix = fitz.Pixmap(doc, xref)
                        sizes.append(len(pix.samples))
                        pix = None
                    except Exception:
                        continue
            doc.close()

        elif ext == '.docx':
            try:
                doc = docx.Document(file_path)
                for rel in list(doc.part.rels.values()):
                    if len(sizes) >= sample_limit:
                        break
                    if "image" in rel.reltype:
                        try:
                            blob = rel.target_part.blob
                            sizes.append(len(blob))
                        except Exception:
                            continue
            except Exception:
                pass

        elif ext == '.pptx':
            try:
                with ZipFile(file_path) as zipf:
                    media_files = [name for name in zipf.namelist() if name.startswith('ppt/media/')]
                    for media in media_files[:sample_limit]:
                        try:
                            b = zipf.read(media)
                            sizes.append(len(b))
                        except Exception:
                            continue
            except Exception:
                pass

        elif ext in ('.html', '.htm'):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    soup = BeautifulSoup(f, 'html.parser')
                img_tags = soup.find_all('img')[:sample_limit]
                for tag in img_tags:
                    src = (tag.get('src') or tag.get('data-src') or tag.get('data-original') or tag.get('data-srcset') or '').strip()
                    if not src:
                        continue
                    # try HEAD for remote images to get content-length
                    if src.startswith('//'):
                        src = 'https:' + src
                    if src.startswith('http'):
                        try:
                            head = requests.head(src, allow_redirects=True, timeout=3)
                            cl = head.headers.get('content-length')
                            if cl and cl.isdigit():
                                sizes.append(int(cl))
                                continue
                        except Exception:
                            pass
                        try:
                            resp = requests.get(src, stream=True, timeout=5)
                            if resp.status_code == 200:
                                sizes.append(len(resp.content))
                        except Exception:
                            continue
                    else:
                        local_path = os.path.join(os.path.dirname(file_path), src)
                        if os.path.exists(local_path):
                            try:
                                with open(local_path, 'rb') as lf:
                                    data = lf.read()
                                    sizes.append(len(data))
                            except Exception:
                                continue
            except Exception:
                pass

    except Exception:
        sizes = []

    # mode -> multiplier: lower multiplier means lower threshold (keep smaller images)
    if mode == 'aggressive':
        multiplier = 0.25
    elif mode == 'conservative':
        multiplier = 1.2
    else:  # balanced
        multiplier = 0.5

    if sizes:
        med = int(statistics.median(sizes))
        computed = max(int(med * multiplier), min_absolute)
        # prevent extreme thresholds
        computed = int(max(min_absolute, min(computed, max(200000, med * 4))))
        return computed
    else:
        fallback_map = {
            '.pdf': 50000,
            '.docx': 15000,
            '.pptx': 20000,
            '.html': 8000
        }
        return fallback_map.get(ext, min_absolute)


def process_document(file_path, determine_mode='balanced'):
    """
    Patched process_document that:
      - supports local paths or http/https URLs (via existing _download_url_to_tempfile)
      - computes a dynamic min_size using determine_min_size(...) and passes it to extractors
      - saves images under extracted_images_<type>/<filename>/ and text output inside that folder
    """
    temp_file_to_remove = None
    is_url = isinstance(file_path, str) and file_path.lower().startswith(('http://', 'https://'))
    original_url = file_path if is_url else None

    if is_url:
        print(f"Detected URL. Downloading: {file_path}")
        try:
            tmp_path, tmp_ext = _download_url_to_tempfile(file_path)
        except Exception as e:
            raise ValueError(f"Failed to download URL: {file_path}. Error: {e}")
        temp_file_to_remove = tmp_path
        file_path_to_use = tmp_path
    else:
        file_path_to_use = file_path

    try:
        ext = os.path.splitext(file_path_to_use)[1].lower()

        base_output_dir = f"extracted_images_{ext[1:] if ext else 'unknown'}"

        if is_url:
            parsed = urlparse(file_path)
            url_basename = os.path.basename(parsed.path) or parsed.netloc
            file_folder = os.path.splitext(url_basename)[0]
            file_folder = "".join(c for c in file_folder if c.isalnum() or c in ('_', '-')).strip() or "downloaded_file"
        else:
            file_folder = os.path.splitext(os.path.basename(file_path_to_use))[0]

        output_dir = os.path.join(base_output_dir, file_folder)
        os.makedirs(output_dir, exist_ok=True)

        output_file = os.path.join(output_dir, f"extracted_texts_{file_folder}.txt")

        # compute adaptive threshold
        min_size = determine_min_size(file_path_to_use, ext, mode=determine_mode, sample_limit=8, min_absolute=5000)
        print(f"Using min_size={min_size} bytes for extraction (mode={determine_mode}).")

        # call extractors with min_size
        if ext == '.pdf':
            image_data = extract_figures_from_pdf(file_path_to_use, output_dir, min_size=min_size)
            doc = fitz.open(file_path_to_use)
        elif ext == '.docx':
            image_data = extract_figures_from_docx(file_path_to_use, output_dir, min_size=min_size)
            doc = docx.Document(file_path_to_use)
        elif ext == '.pptx':
            image_data = extract_figures_from_pptx(file_path_to_use, output_dir, min_size=min_size)
            doc = Presentation(file_path_to_use)
        elif ext in ('.html', '.htm'):
            # If you have an improved HTML extractor that accepts page_url, change the call accordingly:
            # image_data = extract_figures_from_html(file_path_to_use, output_dir, min_size=min_size, page_url=original_url)
            image_data = extract_figures_from_html(file_path_to_use, output_dir, min_size=min_size)
            with open(file_path_to_use, 'r', encoding='utf-8') as f:
                doc = BeautifulSoup(f, 'html.parser')
        else:
            # fallback: try to treat as HTML
            try:
                with open(file_path_to_use, 'r', encoding='utf-8') as f:
                    _ = f.read(2048)
                ext = '.html'
                image_data = extract_figures_from_html(file_path_to_use, output_dir, min_size=min_size)
                with open(file_path_to_use, 'r', encoding='utf-8') as f:
                    doc = BeautifulSoup(f, 'html.parser')
            except Exception:
                raise ValueError(f"Unsupported or unrecognized file type for: {file_path}")

        # Process images and save results
        with open(output_file, "w", encoding="utf-8") as out:
            for img_path, img_bytes, img_ext, context_id in image_data:
                print(f"\nProcessing: {img_path}")
                context_text = get_context(doc, context_id, ext)
                description = gemini_describe(img_bytes, img_ext, context_text)
                out.write(f"{img_path}:\n{description}\n\n")

        if ext == '.pdf':
            doc.close()

        print(f"\n✅ All results saved to: {output_file}")
        print(f"✅ Extracted images saved under: {output_dir}")

    finally:
        if temp_file_to_remove and os.path.exists(temp_file_to_remove):
            try:
                os.remove(temp_file_to_remove)
            except Exception:
                pass


# ======================
# Run
# ======================
process_document(DOC_PATH)
