import hashlib
import unicodedata
from pathlib import Path

def _slugify(value: str, max_len: int = 80) -> str:
    """
    Create safe filenames from link text or titles.
    """
    if not value:
        return ""
    value = unicodedata.normalize("NFKD", value).encode("ascii", "ignore").decode("ascii")
    value = "".join(c if c.isalnum() or c in "-_." else "-" for c in value.strip())
    value = "-".join(filter(None, value.split("-")))
    if len(value) > max_len:
        value = value[:max_len].rstrip("-")
    return value or ""


def _zpad(n: int, width: int = 4) -> str:
    return f"{n:0{width}d}"


class DownloadConfluenceLinks:
    # ... your existing __init__ and methods ...

    def _fetch_html(self, url: str, timeout: int = 30) -> tuple[str | None, dict]:
        """
        Download raw HTML for a URL using the shared session.
        """
        try:
            resp = self.session.get(url, timeout=timeout, verify=self.verify)
            if resp.status_code >= 400:
                return None, {"status": resp.status_code}
            return resp.text, {
                "status": resp.status_code,
                "content_type": resp.headers.get("Content-Type", ""),
                "final_url": str(resp.url),
            }
        except Exception as e:
            return None, {"error": repr(e)}

    def _snapshot_links_to_local(self, results: dict, base_folder: str = "snapshots") -> None:
        """
        Save HTML pages locally in structured folders, preserving link order.
        """
        page_id = results.get("page_id") or self.page_id or "unknown"
        root = Path(base_folder) / f"page-{page_id}"
        root.mkdir(parents=True, exist_ok=True)

        links_root = results.get("links", {}) if isinstance(results, dict) else {}
        categories = [
            ("confluence", links_root.get("confluence", [])),
            ("external",   links_root.get("external", [])),
        ]

        nested = results.get("nested_links") or links_root.get("nested") or []
        if nested:
            categories.append(("nested", nested))

        for cat_name, items in categories:
            if not isinstance(items, list) or not items:
                continue

            cat_dir = root / cat_name
            cat_dir.mkdir(parents=True, exist_ok=True)

            for idx, item in enumerate(items, start=1):
                url = (item.get("url") if isinstance(item, dict) else None) or ""
                if not url:
                    continue

                html_text, meta = self._fetch_html(url)
                if not html_text:
                    continue

                title = ""
                if isinstance(item, dict):
                    title = item.get("text") or item.get("title") or ""
                if not title:
                    try:
                        domain = urlparse(url).netloc
                    except Exception:
                        domain = ""
                    fallback = domain or hashlib.sha1(url.encode("utf-8")).hexdigest()[:8]
                    title = fallback

                filename = f"{_zpad(idx)}_{_slugify(title)}.html"
                filepath = cat_dir / filename

                filepath.write_text(html_text, encoding="utf-8", errors="ignore")


# Existing step: results built + maybe JSON saved
# with open(json_path, "w", encoding="utf-8") as f:
#     json.dump(results, f, ensure_ascii=False, indent=2)

# NEW: save HTML snapshots locally
self._snapshot_links_to_local(results, base_folder="snapshots")
