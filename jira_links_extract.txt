#!/usr/bin/env python3
"""
jira_extract_links_json.py

Usage:
  1) pip install requests
  2) Put one JIRA URL or issue key per line in issue_list.txt
  3) Edit JIRA_BASE, EMAIL, API_TOKEN
  4) python jira_extract_links_json.py
"""

import re
import requests
import os
import json
from urllib.parse import urlparse

# ---------- CONFIG ----------
JIRA_BASE = "https://yourcompany.atlassian.net"   # e.g. https://myorg.atlassian.net
EMAIL = "your.email@company.com"
API_TOKEN = "YOUR_API_TOKEN"
ISSUE_LIST_FILE = "issue_list.txt"   # one JIRA URL or key per line
OUTPUT_JSON = "found_links.json"
# ----------------------------

URL_RE = re.compile(r"https?://[^\s\)\]\}>\'\"`]+", re.IGNORECASE)

def issue_key_from_line(line: str) -> str:
    line = line.strip()
    if not line:
        return ""
    if "browse" in line:
        return line.split("/")[-1]
    return line

def get_issue(issue_key: str, session: requests.Session):
    url = f"{JIRA_BASE}/rest/api/2/issue/{issue_key}?expand=renderedFields,changelog"
    resp = session.get(url, timeout=30)
    resp.raise_for_status()
    return resp.json()

def get_comments(issue_key: str, session: requests.Session):
    comments = []
    start = 0
    maxResults = 50
    while True:
        url = f"{JIRA_BASE}/rest/api/2/issue/{issue_key}/comment?startAt={start}&maxResults={maxResults}"
        r = session.get(url, timeout=30)
        r.raise_for_status()
        data = r.json()
        comments.extend(data.get("comments", []))
        if start + maxResults >= data.get("total", 0):
            break
        start += maxResults
    return comments

def extract_links(text: str):
    if not text:
        return []
    found = URL_RE.findall(str(text))
    cleaned = [u.rstrip(".,;:)\"'>]}") for u in found]
    return list(dict.fromkeys(cleaned))  # dedupe

def main():
    if not os.path.exists(ISSUE_LIST_FILE):
        print(f"Input file '{ISSUE_LIST_FILE}' not found.")
        return

    session = requests.Session()
    session.auth = (EMAIL, API_TOKEN)
    session.headers.update({"Accept": "application/json"})

    with open(ISSUE_LIST_FILE, "r", encoding="utf-8") as f:
        issue_keys = [issue_key_from_line(l) for l in f if l.strip()]

    print(f"Processing {len(issue_keys)} issues...")

    results = {}

    for ik in issue_keys:
        print(f"Fetching {ik} ...")
        try:
            data = get_issue(ik, session)
        except Exception as e:
            print(f"  ERROR: {e}")
            continue

        fields = data.get("fields", {})
        issue_result = {"description": [], "comments": [], "other_fields": [], "attachments": []}

        # description
        issue_result["description"] = extract_links(fields.get("description", ""))

        # other fields (environment, customfields, etc.)
        for fname, fval in fields.items():
            if fname in ("description", "comment", "attachment"):
                continue
            links = extract_links(str(fval))
            if links:
                issue_result["other_fields"].extend(links)

        # attachments
        for att in fields.get("attachment", []) or []:
            if att.get("content"):
                issue_result["attachments"].append(att["content"])

        # comments
        try:
            comments = get_comments(ik, session)
            for c in comments:
                links = extract_links(c.get("body", ""))
                if links:
                    issue_result["comments"].extend(links)
        except Exception as e:
            print(f"  WARNING fetching comments: {e}")

        # dedupe inside each section
        for k in issue_result:
            issue_result[k] = list(dict.fromkeys(issue_result[k]))

        results[ik] = issue_result

    # save JSON
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"Done. Saved results to {OUTPUT_JSON}")

if __name__ == "__main__":
    main()
